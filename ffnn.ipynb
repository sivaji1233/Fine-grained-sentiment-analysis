{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"combined_FFNN_for_part3.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SW37nqQyLORQ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.nn import init\n","import torch.optim as optim\n","import math\n","import random\n","import os\n","from pathlib import Path \n","import time\n","from tqdm import tqdm\n","import json\n","# from tqdm import tqdm_notebook as tqdm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DswvGR0NL9EE","colab_type":"code","colab":{}},"source":["def fetch_data():\n","\twith open('training.json') as training_f:\n","\t\ttraining = json.load(training_f)\n","\twith open('validation.json') as valid_f:\n","\t\tvalidation = json.load(valid_f)\n","\t# If needed you can shrink the training and validation data to speed up somethings but this isn't always safe to do by setting k < 16000\n","\t# k = #fill in\n","\t# training = random.shuffle(training)\n","\t# validation = random.shuffle(validation)\n","\t# training, validation = training[:k], validation[:(k // 10)]\n","\ttra = []\n","\tval = []\n","\tfor elt in training:\n","\t\ttra.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n","\tfor elt in validation:\n","\t\tval.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n","    \n","\treturn tra, val"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e_tnIge5a7W_","colab_type":"code","colab":{}},"source":["# train, valid = fetch_data()\n","# valid[99]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pefBhGiAMA4v","colab_type":"code","colab":{}},"source":["unk = '<UNK>'\n","# Consult the PyTorch documentation for information on the functions used below:\n","# https://pytorch.org/docs/stable/torch.html\n","\n","class FFNN(nn.Module):\n","    def __init__(self, input_dim, h):\n","            super(FFNN, self).__init__()\n","            self.h = h\n","            self.W1 = nn.Linear(input_dim, h)\n","            self.activation = nn.ReLU()\n","            # self.activation = nn.Hardtanh()\n","            # self.activation = nn.LogSigmoid() # The rectified linear unit; one valid choice of activation function\n","            self.W2 = nn.Linear(h, 5)  #Changed\n","            # The below two lines are not a source for an error\n","            self.softmax = nn.LogSoftmax() # The softmax function that converts vectors into probability distributions; computes log probabilities for computational benefits\n","            self.loss = nn.NLLLoss() # The cross-entropy/negative log likelihood loss taught in class\n","\n","    def compute_Loss(self, predicted_vector, gold_label):\n","        return self.loss(predicted_vector, gold_label)\n","\n","    def forward(self, input_vector):\n","        # The z_i are just there to record intermediary computations for your clarity\n","        z1 = self.W1(input_vector)\n","        z3 = self.activation(z1)            # CHANGED\n","        z2 = self.W2(z3)\n","        predicted_vector = self.softmax(self.activation(z2))\n","        return predicted_vector\n","\n","\n","# Returns: \n","# vocab = A set of strings corresponding to the vocabulary\n","def make_vocab(data):\n","    vocab = set()\n","    for document, _ in data:\n","        for word in document:\n","            vocab.add(word)\n","    return vocab \n","\n","\n","# Returns:\n","# vocab = A set of strings corresponding to the vocabulary including <UNK>\n","# word2index = A dictionary mapping word/token to its index (a number in 0, ..., V - 1)\n","# index2word = A dictionary inverting the mapping of word2index\n","def make_indices(vocab):\n","    vocab_list = sorted(vocab)\n","    vocab_list.append(unk)\n","    word2index = {}\n","    index2word = {}\n","    for index, word in enumerate(vocab_list):\n","        word2index[word] = index \n","        index2word[index] = word \n","    vocab.add(unk)\n","    return vocab, word2index, index2word \n","\n","\n","# Returns:\n","# vectorized_data = A list of pairs (vector representation of input, y)\n","def convert_to_vector_representation(data, word2index):\n","    vectorized_data = []\n","    for document, y in data:\n","        vector = torch.zeros(len(word2index)) \n","        for word in document:\n","            index = word2index.get(word, word2index[unk])\n","            vector[index] += 1\n","        vectorized_data.append((vector, y))\n","    return vectorized_data\n","\n","\n","def main(hidden_dim, number_of_epochs):\n","     print(\"Fetching data\")\n","     train_data, valid_data = fetch_data() # X_data is a list of pairs (document, y); y in {0,1,2,3,4}\n","     vocab = make_vocab(train_data)\n","     vocab, word2index, index2word = make_indices(vocab)\n","     print(\"Fetched and indexed data\")\n","     train_data = convert_to_vector_representation(train_data, word2index) # vocab corresponding to unk will always be zero.\n","     valid_data = convert_to_vector_representation(valid_data, word2index)\n","     print(\"Vectorized data\")\n","    #  print(len(train_data[0][0]))\n","    #  print(len(valid_data[0][0]))    \n","     \n","     model = FFNN(input_dim = len(vocab), h = hidden_dim)\n","     model = model.cuda()\n","    #  optimizer = optim.SGD(model.parameters(),lr=0.01, momentum=0.9)\n","     optimizer = optim.Adam(model.parameters())\n","     print(\"Training for {} epochs\".format(number_of_epochs))\n","     for epoch in range(number_of_epochs):\n","         model.train()\n","        #  optimizer.zero_grad()    # CHANGED\n","         loss = None\n","         correct = 0\n","         total = 0\n","         start_time = time.time()\n","         print(\"Training started for epoch {}\".format(epoch + 1))\n","         random.shuffle(train_data) # Good practice to shuffle order of training data\n","         minibatch_size = 16 \n","         N = len(train_data) \n","         for minibatch_index in tqdm(range(N // minibatch_size), position=0, leave=False):\n","             optimizer.zero_grad()\n","             loss = None\n","             random_minibatch = random.sample(range(minibatch_size), minibatch_size)   #CHANGED\n","            #  for example_index in range(minibatch_size):\n","             for example_index in random_minibatch:\n","                 input_vector, gold_label = train_data[minibatch_index * minibatch_size + example_index]\n","                 input_vector = input_vector.cuda()\n","                 gold_label = torch.tensor([gold_label]).cuda()\n","                 predicted_vector = model(input_vector)\n","                 predicted_label = torch.argmax(predicted_vector)\n","                #  print(predicted_vector)\n","                #  print(predicted_label)\n","                #  print(torch.tensor(gold_label))\n","                #  break\n","                 correct += int(predicted_label == gold_label)\n","                 total += 1\n","                 example_loss = model.compute_Loss(predicted_vector.view(1,-1), gold_label) # torch.tensor([gold_label])\n","                 if loss is None:\n","                     loss = example_loss\n","                 else:\n","                     loss += example_loss\n","\n","             loss = loss / minibatch_size      #CHANGED\n","             loss = loss.cuda()\n","             loss.backward()\n","             optimizer.step()\n","         print(\"Training completed for epoch {}\".format(epoch + 1))\n","         print(\"Training accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n","         print(\"Training time for this epoch: {}\".format(time.time() - start_time))\n","         \n","         loss = None\n","         correct = 0\n","         total = 0\n","         start_time = time.time()\n","         print(\"Validation started for epoch {}\".format(epoch + 1))\n","         random.shuffle(valid_data) # Good practice to shuffle order of validation data\n","         minibatch_size = 16 \n","         N = len(valid_data)\n","         with torch.no_grad(): # CHANGED\n","          # model.eval()           \n","          for minibatch_index in tqdm(range(N // minibatch_size), position=0, leave=False):\n","              optimizer.zero_grad()     # CHANGED\n","              loss = None\n","              for example_index in range(minibatch_size):\n","                  input_vector, gold_label = valid_data[minibatch_index * minibatch_size + example_index]\n","                  input_vector = input_vector.cuda()\n","                  gold_label = torch.tensor([gold_label]).cuda()\n","                  predicted_vector = model(input_vector)\n","                  predicted_label = torch.argmax(predicted_vector)\n","                  # if predicted_label == gold_label:\n","                  #   print(minibatch_index * minibatch_size + example_index)\n","                  correct += int(predicted_label == gold_label)\n","                  total += 1\n","                  example_loss = model.compute_Loss(predicted_vector.view(1,-1), gold_label)\n","                  if loss is None:\n","                      loss = example_loss\n","                  else:\n","                      loss += example_loss\n","              loss = loss / minibatch_size\n","              loss = loss.cuda()\n","              # loss.backward()                # CHANGED\n","              # optimizer.step()               # CHANGED\n","          print(\"Validation completed for epoch {}\".format(epoch + 1))\n","          print(\"Validation accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n","          print(\"Validation time for this epoch: {}\".format(time.time() - start_time))\n","          print(\"the correct number {}\".format(correct))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3nuNkGGpG0EI","colab_type":"code","outputId":"4f439a2f-c004-4450-fe3c-770fd91e7a63","executionInfo":{"status":"ok","timestamp":1573531508501,"user_tz":300,"elapsed":55883,"user":{"displayName":"sivaji gowtham","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCrvJeC3f8MZRam3tkXmI6Xm50fU17BqQBAp3a5ZA=s64","userId":"09227610564193230075"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["hidden_dim = 32\n","number_of_epochs = 1\n","main(hidden_dim=hidden_dim, number_of_epochs=number_of_epochs)\n","# if __name__ == '__main__':\n","# \tmain()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Fetching data\n","Fetched and indexed data\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  0%|          | 5/1000 [00:00<00:23, 41.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Vectorized data\n","Training for 1 epochs\n","Training started for epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["  6%|▌         | 6/100 [00:00<00:01, 50.25it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training completed for epoch 1\n","Training accuracy for epoch 1: 0.3985625\n","Training time for this epoch: 23.041085720062256\n","Validation started for epoch 1\n","0\n","2\n","5\n","11\n","13\n","16\n","17\n","20\n","22\n","23\n","24\n","26\n","34\n","40\n","41\n","43\n","45\n","51\n","55\n","62\n","70\n","71\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","86\n","87\n","91\n","93\n","94\n","99\n","100\n","103\n","104\n","105\n","107\n","108\n","109\n","110\n","111\n","114\n","115\n","116\n","118\n","119\n","120\n","126\n","127\n","128\n","129\n","134\n","137\n","138\n","139\n","141\n","142\n","144\n","146\n","148\n","152\n","154\n","156\n","157\n"],"name":"stdout"},{"output_type":"stream","text":[" 16%|█▌        | 16/100 [00:00<00:01, 49.74it/s]"],"name":"stderr"},{"output_type":"stream","text":["159\n","161\n","162\n","164\n","165\n","176\n","177\n","180\n","183\n","187\n","188\n","189\n","191\n","193\n","195\n","196\n","198\n","199\n","200\n","201\n","205\n","206\n","209\n","210\n","212\n","217\n","218\n","222\n","223\n","226\n","227\n","232\n","233\n","237\n","238\n","239\n","240\n","241\n","245\n","247\n","248\n","250\n","259\n","262\n","263\n","264\n","265\n","266\n","268\n","276\n","278\n","281\n","282\n","289\n","295\n","296\n","303\n","304\n","305\n","306\n","307\n","309\n","311\n","315\n","317\n"],"name":"stdout"},{"output_type":"stream","text":[" 26%|██▌       | 26/100 [00:00<00:01, 49.13it/s]"],"name":"stderr"},{"output_type":"stream","text":["320\n","322\n","324\n","327\n","330\n","331\n","332\n","336\n","338\n","340\n","341\n","345\n","348\n","350\n","351\n","352\n","354\n","359\n","360\n","361\n","363\n","364\n","366\n","368\n","370\n","373\n","375\n","376\n","378\n","380\n","382\n","387\n","389\n","390\n","392\n","393\n","394\n","395\n","400\n","405\n","406\n","409\n","410\n","411\n","413\n","414\n","415\n","421\n","424\n","427\n","428\n","430\n","433\n","435\n","436\n","442\n","443\n","444\n","446\n","449\n","450\n","451\n","452\n","457\n","458\n","461\n","462\n","463\n","464\n","466\n","471\n","472\n","474\n"],"name":"stdout"},{"output_type":"stream","text":[" 36%|███▌      | 36/100 [00:00<00:01, 48.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["478\n","479\n","480\n","483\n","485\n","496\n","498\n","502\n","503\n","504\n","505\n","506\n","512\n","514\n","517\n","521\n","524\n","525\n","527\n","529\n","538\n","540\n","541\n","542\n","544\n","546\n","547\n","551\n","553\n","554\n","556\n","557\n","558\n","560\n","561\n","562\n","569\n","570\n","571\n","572\n","573\n","574\n","575\n","577\n","578\n","579\n","580\n","582\n","591\n","592\n","596\n","597\n","598\n","602\n","604\n","606\n","607\n","608\n","609\n","611\n","612\n","614\n","615\n","620\n","621\n","623\n","625\n","627\n"],"name":"stdout"},{"output_type":"stream","text":[" 46%|████▌     | 46/100 [00:00<00:01, 48.38it/s]"],"name":"stderr"},{"output_type":"stream","text":["633\n","638\n","640\n","643\n","644\n","646\n","650\n","651\n","655\n","656\n","657\n","660\n","663\n","665\n","671\n","672\n","678\n","680\n","681\n","682\n","686\n","687\n","688\n","690\n","693\n","695\n","696\n","697\n","698\n","702\n","706\n","707\n","708\n","709\n","710\n","711\n","717\n","718\n","721\n","722\n","723\n","725\n","726\n","727\n","728\n","729\n","732\n","733\n","734\n","736\n","738\n","739\n","741\n","749\n","750\n","751\n","752\n","755\n","756\n","758\n","761\n","765\n","770\n","774\n","776\n","777\n","779\n","781\n","783\n","785\n","787\n"],"name":"stdout"},{"output_type":"stream","text":[" 56%|█████▌    | 56/100 [00:01<00:00, 48.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["793\n","800\n","801\n","803\n","810\n","811\n","815\n","816\n","818\n","820\n","824\n","825\n","828\n","832\n","833\n","834\n","835\n","842\n","845\n","846\n","849\n","850\n","852\n","856\n","857\n","858\n","861\n","862\n","867\n","878\n","879\n","882\n","884\n","885\n","890\n","892\n","894\n","896\n","898\n","899\n","905\n","911\n","913\n","914\n","917\n","920\n","922\n","923\n","925\n","926\n","928\n","930\n","937\n","940\n","945\n","951\n"],"name":"stdout"},{"output_type":"stream","text":[" 66%|██████▌   | 66/100 [00:01<00:00, 49.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["953\n","956\n","960\n","961\n","963\n","966\n","967\n","971\n","973\n","974\n","975\n","978\n","979\n","981\n","983\n","986\n","987\n","988\n","989\n","992\n","996\n","997\n","999\n","1001\n","1005\n","1009\n","1010\n","1011\n","1013\n","1014\n","1017\n","1019\n","1021\n","1022\n","1025\n","1026\n","1027\n","1031\n","1036\n","1039\n","1040\n","1042\n","1045\n","1046\n","1047\n","1052\n","1053\n","1054\n","1056\n","1060\n","1062\n","1063\n","1066\n","1067\n","1068\n","1069\n","1074\n","1078\n","1080\n","1081\n","1083\n","1085\n","1086\n","1089\n","1090\n","1091\n","1092\n","1094\n","1095\n","1098\n","1099\n","1100\n","1101\n","1102\n","1103\n","1106\n","1107\n","1109\n"],"name":"stdout"},{"output_type":"stream","text":[" 76%|███████▌  | 76/100 [00:01<00:00, 49.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["1110\n","1112\n","1114\n","1115\n","1123\n","1125\n","1130\n","1133\n","1134\n","1135\n","1136\n","1137\n","1138\n","1140\n","1143\n","1144\n","1146\n","1149\n","1153\n","1155\n","1156\n","1165\n","1166\n","1168\n","1170\n","1171\n","1172\n","1175\n","1178\n","1180\n","1186\n","1189\n","1193\n","1196\n","1197\n","1199\n","1202\n","1210\n","1213\n","1214\n","1215\n","1216\n","1218\n","1219\n","1222\n","1226\n","1237\n","1241\n","1242\n","1244\n","1246\n","1247\n","1250\n","1251\n","1255\n","1256\n","1258\n","1263\n"],"name":"stdout"},{"output_type":"stream","text":[" 86%|████████▌ | 86/100 [00:01<00:00, 48.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["1269\n","1271\n","1279\n","1282\n","1283\n","1285\n","1288\n","1289\n","1290\n","1291\n","1292\n","1294\n","1295\n","1298\n","1300\n","1305\n","1306\n","1308\n","1310\n","1312\n","1315\n","1316\n","1317\n","1318\n","1323\n","1326\n","1327\n","1328\n","1333\n","1338\n","1341\n","1343\n","1344\n","1348\n","1349\n","1350\n","1351\n","1354\n","1356\n","1359\n","1364\n","1366\n","1367\n","1368\n","1370\n","1373\n","1374\n","1375\n","1376\n","1377\n","1378\n","1379\n","1382\n","1384\n","1385\n","1388\n","1389\n","1391\n","1394\n","1399\n","1400\n","1405\n","1406\n","1410\n","1412\n","1414\n","1416\n","1417\n","1419\n","1422\n","1423\n","1424\n"],"name":"stdout"},{"output_type":"stream","text":[" 96%|█████████▌| 96/100 [00:01<00:00, 48.74it/s]"],"name":"stderr"},{"output_type":"stream","text":["1425\n","1431\n","1432\n","1434\n","1435\n","1437\n","1439\n","1440\n","1441\n","1442\n","1443\n","1444\n","1445\n","1455\n","1461\n","1462\n","1465\n","1466\n","1467\n","1468\n","1469\n","1471\n","1472\n","1474\n","1478\n","1482\n","1483\n","1484\n","1485\n","1487\n","1489\n","1497\n","1500\n","1501\n","1503\n","1504\n","1505\n","1509\n","1512\n","1513\n","1516\n","1517\n","1518\n","1519\n","1523\n","1524\n","1525\n","1526\n","1529\n","1533\n","1534\n","1535\n","1540\n","1542\n","1546\n","1549\n","1551\n","1553\n","1554\n","1560\n","1563\n","1564\n","1566\n","1571\n","1573\n","1574\n","1576\n"],"name":"stdout"},{"output_type":"stream","text":[""],"name":"stderr"},{"output_type":"stream","text":["1585\n","1586\n","1587\n","1590\n","1591\n","1593\n","1594\n","Validation completed for epoch 1\n","Validation accuracy for epoch 1: 0.426875\n","Validation time for this epoch: 2.0497145652770996\n","the correct number 683\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"souoQSETGkgz","colab_type":"text"},"source":["Training accuracy for epoch 1: 0.4165 <br>\n","Validation accuracy for epoch 1: 0.490625\n","<br><br>\n","Training accuracy for epoch 10: 0.703875<br>\n","Validation accuracy for epoch 10: 0.693125\n","<br><br>\n","Training accuracy for epoch 10: 0.7051875<br>\n","Validation accuracy for epoch 10: 0.53125"]},{"cell_type":"code","metadata":{"id":"yVVrRabmGyn_","colab_type":"code","outputId":"0c4fba48-06e3-4f00-b243-d65360ad7e7f","executionInfo":{"status":"ok","timestamp":1573237111142,"user_tz":300,"elapsed":486,"user":{"displayName":"Abhishek Sarkar","photoUrl":"","userId":"04556928578932398482"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# for i in tqdm(range(4)):\n","#   print(i)\n","random.sample(range(10),10)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[9, 0, 2, 5, 8, 7, 6, 1, 4, 3]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"mulYCLIWXxQz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}