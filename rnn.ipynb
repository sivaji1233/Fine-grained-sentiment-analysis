{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"combined_RNN_abs.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SW37nqQyLORQ","colab_type":"code","colab":{}},"source":["# To run this code please upload files training.json, validation.json, glove.6B.300d.txt on the current\n","# google files and run it using GPU\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.nn import init\n","import torch.optim as optim\n","import math\n","import random\n","import os\n","from pathlib import Path \n","import time\n","from tqdm import tqdm\n","import json\n","from gensim.models import Word2Vec\n","# from tqdm import tqdm_notebook as tqdm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EvomKWAxWCBb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"28837b21-9504-4042-b147-7326ed3bebd3","executionInfo":{"status":"ok","timestamp":1573529201802,"user_tz":300,"elapsed":12885,"user":{"displayName":"Abhishek Sarkar","photoUrl":"","userId":"04556928578932398482"}}},"source":["!unzip 'glove.6B.300d.zip'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Archive:  glove.6B.300d.zip\n","  inflating: glove.6B.300d.txt       \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DswvGR0NL9EE","colab_type":"code","colab":{}},"source":["def fetch_data():\n","\twith open('training.json') as training_f:\n","\t\ttraining = json.load(training_f)\n","\twith open('validation.json') as valid_f:\n","\t\tvalidation = json.load(valid_f)\n","\t# If needed you can shrink the training and validation data to speed up somethings but this isn't always safe to do by setting k < 16000\n","\t# k = #fill in\n","\t# training = random.shuffle(training)\n","\t# validation = random.shuffle(validation)\n","\t# training, validation = training[:k], validation[:(k // 10)]\n","\ttra = []\n","\tval = []\n","\tfor elt in training:\n","\t\ttra.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n","\tfor elt in validation:\n","\t\tval.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n","    \n","\treturn tra, val"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iT694tlCSxTQ","colab_type":"code","colab":{}},"source":["# Returns: \n","# vocab = A set of strings corresponding to the vocabulary\n","def make_vocab(data):\n","    vocab = set()\n","    for document, _ in data:\n","        for word in document:\n","            vocab.add(word)\n","    return vocab "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rhq1BQWuS23w","colab_type":"code","colab":{}},"source":["# Returns:\n","# vocab = A set of strings corresponding to the vocabulary including <UNK>\n","# word2index = A dictionary mapping word/token to its index (a number in 0, ..., V - 1)\n","# index2word = A dictionary inverting the mapping of word2index\n","def make_indices(vocab):\n","    vocab_list = sorted(vocab)\n","    vocab_list.append(unk)\n","    word2index = {}\n","    index2word = {}\n","    for index, word in enumerate(vocab_list):\n","        word2index[word] = index \n","        index2word[index] = word \n","    # vocab.add(unk)\n","    return vocab, word2index, index2word "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-kP1MHfS_bD","colab_type":"code","colab":{}},"source":["# Returns:\n","# vectorized_data = A list of pairs (vector representation of input, y)\n","def convert_to_vector_representation(data, word2index):\n","    vectorized_data = []\n","    for document, y in data:\n","        vector = torch.zeros(len(word2index)) \n","        for word in document:\n","            index = word2index.get(word, word2index[unk])\n","            vector[index] += 1\n","        vectorized_data.append((vector, y))\n","    return vectorized_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hne-jnf9mXKz","colab_type":"code","colab":{}},"source":["# Returns:\n","# glove_dict = {'word': vector} # for all the words\n","def glove_dict_generation():\n","    glove_dict = {}\n","    with open('glove.6B.300d.txt', 'rb') as word_corpus:\n","      for item in word_corpus:\n","        item = item.decode().split()\n","        word = item[0]\n","        vect = np.array(item[1:]).astype(np.float)\n","        if word not in glove_dict:\n","          glove_dict[word] = vect\n","        \n","    return glove_dict\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"69l1ubjT056J","colab_type":"code","colab":{}},"source":["# Returns:\n","# embedding_matrix = row(word tokens), cols(embedding dimension)\n","def embedding_matrix_per_document(input_document, glove_dict):\n","    \n","    embed_dim = len(glove_dict['the'])\n","    embedding_matrix = np.zeros((len(input_document), embed_dim))\n","    count = 0\n","    for i, item in enumerate(input_document):\n","      if item in glove_dict:\n","        embedding_matrix[i] = np.asarray(glove_dict[item])\n","      else:\n","        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embed_dim, ))\n","        count += 1\n","    \n","    # print(count)\n","    embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float, requires_grad = True).cuda()\n","    return embedding_matrix  \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v_lPQFxeTJGs","colab_type":"code","colab":{}},"source":["unk = '<UNK>'\n","# Consult the PyTorch documentation for information on the functions used below:\n","# https://pytorch.org/docs/stable/torch.html\n","\n","class RNN(nn.Module):\n","\tdef __init__(self, input_dim, h1, h):\n","\t\tsuper(RNN, self).__init__()\n","\t\tself.input_dim = input_dim\n","\t\tself.h = h\n","\t\tself.h1 = h1\n","\t\t# self.rnn = nn.RNNCell(self.input_dim, self.h1)\n","\t\tself.rnn = nn.RNN(input_size=self.input_dim, hidden_size=self.h1, num_layers=1, nonlinearity = 'relu')\n","\t\tself.W1 = nn.Linear(self.h1, self.h)\n","\t\tself.W2 = nn.Linear(self.h, 5)\n","\t\n","\t\tself.activation = nn.ReLU()\t\t\n","\t\tself.softmax = nn.LogSoftmax()\n","\t\tself.loss = nn.NLLLoss()\n","\n","\tdef compute_Loss(self, predicted_vector, gold_label):\n","\t\treturn self.loss(predicted_vector, gold_label)\t\n","\t\n","\tdef init_hidden(self):\n","        # (num_layers, batch_size, n_neurons)\n","\t\t\th = torch.randn(1, 1, self.h1, requires_grad = True, dtype=torch.float)*0.01\n","\t\t\treturn (h.cuda())\n","\n","\tdef forward(self, input_matrix): \n","\t\tn_word = input_matrix.size(0)\n","\t\tn_embed = input_matrix.size(1)\n","\t\tinput_matrix = input_matrix.reshape(n_word, 1, n_embed)\n","\t\tself.hx = self.init_hidden()\n","\t  \n","\t\trnn_out, self.hx = self.rnn(input_matrix, self.hx)     # Explain the architecture.\n","\t\trnn_out = rnn_out.reshape(n_word, self.h1)\n","\t\tself.hx = self.hx.squeeze(0).squeeze(1)\t  \n","\t\th_out = self.hx\n","\t\tz1 = self.W1(h_out)\n","\t\tz3 = self.activation(z1)\n","\t\tz2 = self.W2(z3)\t\n","\t\tpredicted_vector = self.softmax(self.activation(z2))\t\t# is activation required ?\n","\t\treturn predicted_vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pefBhGiAMA4v","colab_type":"code","colab":{}},"source":["def main(hidden_dim, hidden_dim_ffnn, number_of_epochs):\n","     print(\"Fetching data\")\n","     train_data, valid_data = fetch_data() # X_data is a list of pairs (document, y); y in {0,1,2,3,4}\n","     vocab = make_vocab(train_data)\n","     vocab, word2index, index2word = make_indices(vocab)\n","     print(\"Fetched and indexed data\")\n","    #  train_data = convert_to_vector_representation(train_data, word2index) # vocab corresponding to unk will always be zero.\n","    #  valid_data = convert_to_vector_representation(valid_data, word2index)\n","     embed_dim = 300\n","     glove_dict = glove_dict_generation()\n","    #  a = embedding_matrix_per_document(train_data[0][0], glove_dict)\n","     model = RNN(input_dim = embed_dim, h1 = hidden_dim, h = hidden_dim_ffnn).cuda()\n","    \n","     valid_loss_min = np.Inf\n","     epochs_no_improve = 0\n","     max_tol_no_improv_epoch = 5\n","     optimizer = optim.SGD(model.parameters(),lr=0.001, momentum=0.9)\n","     print(\"Training for {} epochs\".format(number_of_epochs))\n","     for epoch in range(number_of_epochs):\n","         model.train()\n","         loss = None\n","         correct = 0\n","         total = 0\n","         start_time = time.time()\n","         print(\"Training started for epoch {}\".format(epoch + 1))\n","         random.shuffle(train_data) # Good practice to shuffle order of training data\n","         minibatch_size = 16 \n","         N = len(train_data) \n","         for minibatch_index in tqdm(range(N // minibatch_size), position=0, leave=False):\n","             optimizer.zero_grad()\n","             loss = None\n","             random_minibatch = random.sample(range(minibatch_size), minibatch_size)   #CHANGED\n","    #         #  for example_index in range(minibatch_size):\n","             for example_index in random_minibatch:\n","                 input_document, gold_label = train_data[minibatch_index * minibatch_size + example_index]\n","                 input_vector = embedding_matrix_per_document(input_document, glove_dict)\n","                 gold_label = torch.tensor([gold_label]).cuda()\n","                 predicted_vector = model(input_vector)\n","                 predicted_label = torch.argmax(predicted_vector)\n","                 correct += int(predicted_label == gold_label)\n","                 total += 1\n","                 example_loss = model.compute_Loss(predicted_vector.view(1,-1), gold_label) # torch.tensor([gold_label])\n","                 if loss is None:\n","                     loss = example_loss\n","                 else:\n","                     loss += example_loss\n","\n","             loss = loss / minibatch_size      #CHANGED\n","             loss = loss.cuda()\n","             loss.backward()\n","             optimizer.step()\n","         print(\"Training completed for epoch {}\".format(epoch + 1))\n","         print(\"Training accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n","         print(\"Training time for this epoch: {}\".format(time.time() - start_time))\n","         \n","         loss = None\n","         correct = 0\n","         total = 0\n","         start_time = time.time()\n","         print(\"Validation started for epoch {}\".format(epoch + 1))\n","         random.shuffle(valid_data) # Good practice to shuffle order of validation data\n","         minibatch_size = 16 \n","         N = len(valid_data)\n","         with torch.no_grad(): \n","            valid_loss = 0\n","            for minibatch_index in tqdm(range(N // minibatch_size), position=0, leave=False):\n","                optimizer.zero_grad()     # CHANGED\n","                loss = None\n","                for example_index in range(minibatch_size):\n","                    input_document, gold_label = valid_data[minibatch_index * minibatch_size + example_index]\n","                    input_vector = embedding_matrix_per_document(input_document, glove_dict)\n","                    gold_label = torch.tensor([gold_label]).cuda()\n","                    predicted_vector = model(input_vector)\n","                    predicted_label = torch.argmax(predicted_vector)\n","        #             # print(predicted_label)\n","                    correct += int(predicted_label == gold_label)\n","                    total += 1\n","                    example_loss = model.compute_Loss(predicted_vector.view(1,-1), gold_label)\n","                    if loss is None:\n","                        loss = example_loss\n","                    else:\n","                        loss += example_loss\n","                \n","                valid_loss += loss\n","                loss = loss / minibatch_size\n","                loss = loss.cuda()\n","        #         # loss.backward()                # Fixed error\n","        #         # optimizer.step()               # Fixed error\n","            valid_loss = valid_loss/N \n","            print(\"Validation completed for epoch {}\".format(epoch + 1))\n","            print(\"Validation accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n","            print(\"Validation time for this epoch: {}\".format(time.time() - start_time))\n","            if valid_loss < valid_loss_min:\n","              valid_loss_min = valid_loss\n","              epochs_no_improve = 0\n","            else:\n","              epochs_no_improve += 1\n","              if epochs_no_improve > max_tol_no_improv_epoch:\n","                print('Early Stopping due to no improvement in Val loss')\n","                break\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3nuNkGGpG0EI","colab_type":"code","outputId":"d11e5298-019d-4df3-dfa6-583185587857","executionInfo":{"status":"ok","timestamp":1573530338125,"user_tz":300,"elapsed":869289,"user":{"displayName":"Abhishek Sarkar","photoUrl":"","userId":"04556928578932398482"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["hidden_dim_ffnn = 32\n","hidden_dim = 32\n","\n","number_of_epochs = 10\n","main(hidden_dim=hidden_dim, hidden_dim_ffnn=hidden_dim_ffnn, number_of_epochs=number_of_epochs)\n","# if __name__ == '__main__':\n","# \tmain()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Fetching data\n","Fetched and indexed data\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  0%|          | 1/1000 [00:00<02:42,  6.14it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training for 10 epochs\n","Training started for epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["  3%|▎         | 3/100 [00:00<00:04, 22.37it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training completed for epoch 1\n","Training accuracy for epoch 1: 0.198625\n","Training time for this epoch: 75.6681604385376\n","Validation started for epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/1000 [00:00<01:11, 14.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation completed for epoch 1\n","Validation accuracy for epoch 1: 0.191875\n","Validation time for this epoch: 4.3912951946258545\n","Training started for epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["  3%|▎         | 3/100 [00:00<00:04, 23.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training completed for epoch 2\n","Training accuracy for epoch 2: 0.202\n","Training time for this epoch: 75.78074216842651\n","Validation started for epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/1000 [00:00<01:08, 14.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation completed for epoch 2\n","Validation accuracy for epoch 2: 0.196875\n","Validation time for this epoch: 4.306215524673462\n","Training started for epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["  3%|▎         | 3/100 [00:00<00:04, 24.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training completed for epoch 3\n","Training accuracy for epoch 3: 0.1989375\n","Training time for this epoch: 75.51424527168274\n","Validation started for epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/1000 [00:00<01:09, 14.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation completed for epoch 3\n","Validation accuracy for epoch 3: 0.19875\n","Validation time for this epoch: 4.303962230682373\n","Training started for epoch 4\n"],"name":"stdout"},{"output_type":"stream","text":["  3%|▎         | 3/100 [00:00<00:04, 21.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training completed for epoch 4\n","Training accuracy for epoch 4: 0.1990625\n","Training time for this epoch: 75.51887035369873\n","Validation started for epoch 4\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/1000 [00:00<01:23, 11.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation completed for epoch 4\n","Validation accuracy for epoch 4: 0.203125\n","Validation time for this epoch: 4.349491119384766\n","Training started for epoch 5\n"],"name":"stdout"},{"output_type":"stream","text":["  3%|▎         | 3/100 [00:00<00:04, 21.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training completed for epoch 5\n","Training accuracy for epoch 5: 0.2006875\n","Training time for this epoch: 76.34521985054016\n","Validation started for epoch 5\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/1000 [00:00<01:25, 11.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation completed for epoch 5\n","Validation accuracy for epoch 5: 0.2025\n","Validation time for this epoch: 4.417638540267944\n","Training started for epoch 6\n"],"name":"stdout"},{"output_type":"stream","text":["  3%|▎         | 3/100 [00:00<00:04, 23.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training completed for epoch 6\n","Training accuracy for epoch 6: 0.204\n","Training time for this epoch: 77.2601466178894\n","Validation started for epoch 6\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/1000 [00:00<01:02, 15.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation completed for epoch 6\n","Validation accuracy for epoch 6: 0.19\n","Validation time for this epoch: 4.408180475234985\n","Training started for epoch 7\n"],"name":"stdout"},{"output_type":"stream","text":["  3%|▎         | 3/100 [00:00<00:03, 25.35it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training completed for epoch 7\n","Training accuracy for epoch 7: 0.199875\n","Training time for this epoch: 78.13465309143066\n","Validation started for epoch 7\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/1000 [00:00<01:53,  8.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation completed for epoch 7\n","Validation accuracy for epoch 7: 0.193125\n","Validation time for this epoch: 4.416424989700317\n","Training started for epoch 8\n"],"name":"stdout"},{"output_type":"stream","text":["  3%|▎         | 3/100 [00:00<00:04, 24.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training completed for epoch 8\n","Training accuracy for epoch 8: 0.202125\n","Training time for this epoch: 76.2076063156128\n","Validation started for epoch 8\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/1000 [00:00<01:07, 14.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation completed for epoch 8\n","Validation accuracy for epoch 8: 0.19375\n","Validation time for this epoch: 4.3585121631622314\n","Training started for epoch 9\n"],"name":"stdout"},{"output_type":"stream","text":["  3%|▎         | 3/100 [00:00<00:05, 19.17it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training completed for epoch 9\n","Training accuracy for epoch 9: 0.1994375\n","Training time for this epoch: 76.18949341773987\n","Validation started for epoch 9\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/1000 [00:00<01:14, 13.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation completed for epoch 9\n","Validation accuracy for epoch 9: 0.20125\n","Validation time for this epoch: 4.33153510093689\n","Training started for epoch 10\n"],"name":"stdout"},{"output_type":"stream","text":["  3%|▎         | 3/100 [00:00<00:04, 23.07it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training completed for epoch 10\n","Training accuracy for epoch 10: 0.20025\n","Training time for this epoch: 75.66520309448242\n","Validation started for epoch 10\n"],"name":"stdout"},{"output_type":"stream","text":[""],"name":"stderr"},{"output_type":"stream","text":["Validation completed for epoch 10\n","Validation accuracy for epoch 10: 0.2025\n","Validation time for this epoch: 4.329193592071533\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"souoQSETGkgz","colab_type":"text"},"source":[""]}]}